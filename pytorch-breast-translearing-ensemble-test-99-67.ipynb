{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **0. Importing Libraries**\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport random\nfrom operator import itemgetter\nimport copy\nimport time\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transform\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.image import imread\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T12:08:51.592233Z","iopub.execute_input":"2022-05-07T12:08:51.592537Z","iopub.status.idle":"2022-05-07T12:08:54.053205Z","shell.execute_reply.started":"2022-05-07T12:08:51.592507Z","shell.execute_reply":"2022-05-07T12:08:54.052224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Data Loading**","metadata":{}},{"cell_type":"markdown","source":"**Paths**","metadata":{}},{"cell_type":"code","source":"example = '../input/mias-mammography/all-mias'\npath = '../input/mias-mammography/all-mias'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:09:02.050126Z","iopub.execute_input":"2022-05-07T12:09:02.050545Z","iopub.status.idle":"2022-05-07T12:09:02.055181Z","shell.execute_reply.started":"2022-05-07T12:09:02.050508Z","shell.execute_reply":"2022-05-07T12:09:02.053878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's take a quick look at the data. I don't know about you, but the first thing I always want to do is look at what our data looks like :)**","metadata":{}},{"cell_type":"code","source":"img = mpimg.imread(example + '/mdb001.pgm')\nprint('Shape:', img.shape)\nplt.imshow(img);","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:09:28.695269Z","iopub.execute_input":"2022-05-07T12:09:28.695631Z","iopub.status.idle":"2022-05-07T12:09:28.994507Z","shell.execute_reply.started":"2022-05-07T12:09:28.695595Z","shell.execute_reply":"2022-05-07T12:09:28.993559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From the image above, you can understand that they are not normalized (which is very expected), how strong the spread can be seen using the script below:**","metadata":{}},{"cell_type":"code","source":"def plotHist(img):\n  plt.figure(figsize=(10,5))\n  plt.subplot(1,2,1)\n  plt.imshow(img)\n  plt.axis('off')\n  histo = plt.subplot(1,2,2)\n  histo.set_ylabel('Count')\n  histo.set_xlabel('Pixel Intensity')\n  plt.hist(img.flatten(), bins=10, lw=0, alpha=0.5, color='r')\n\nplotHist(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:09:36.65631Z","iopub.execute_input":"2022-05-07T12:09:36.656687Z","iopub.status.idle":"2022-05-07T12:09:37.0008Z","shell.execute_reply.started":"2022-05-07T12:09:36.656654Z","shell.execute_reply":"2022-05-07T12:09:36.999898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Data preparation**\n","metadata":{}},{"cell_type":"markdown","source":"**Let's write augmentation and normalization right away. Why are there four datasets?**\n\n**(1)** The \"original\" dataset is the original dataset(wow), which we will split into two parts: test (20%) and training (80%).\n\n**(2)** The dataset \"dataset1\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n\n**(3)** The dataset \"dataset2\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n\n**(4)** The dataset \"dataset3\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n","metadata":{}},{"cell_type":"code","source":"transformer = {\n    'original': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.ToTensor(), \n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))\n]), \n   'dataset1': transform.Compose([\n                           transform.Resize((220, 220)),\n                           transform.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                           transform.RandomRotation(5),\n                           transform.RandomAffine(degrees=11, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transform.ToTensor(),\n                           transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                               (0.3268945515155792, 0.29282665252685547, 0.29053378105163574)),\n]), \n   'dataset2': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.RandomHorizontalFlip(),\n                                 transform.RandomRotation(10),\n                                 transform.RandomAffine(translate=(0.05,0.05), degrees=0),\n                                 transform.ToTensor(),\n                                 transform.RandomErasing(inplace=True, scale=(0.01, 0.23)),\n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))]),\n   'dataset3': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.RandomHorizontalFlip(p=0.5),\n                                 transform.RandomRotation(15),\n                                 transform.RandomAffine(translate=(0.08,0.1), degrees=15),\n                                 transform.ToTensor(),\n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))\n                                                     \n])\n       }","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:09:55.345499Z","iopub.execute_input":"2022-05-07T12:09:55.345859Z","iopub.status.idle":"2022-05-07T12:09:55.357677Z","shell.execute_reply.started":"2022-05-07T12:09:55.345806Z","shell.execute_reply":"2022-05-07T12:09:55.356566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**pin_memory:** You know how sometimes your GPU memory shows that it’s full but you’re pretty sure that your model isn’t using that much? That overhead is called pinned memory. ie: this memory has been reserved as a type of “working allocation.”\nWhen you enable pinned_memory in a DataLoader it “automatically puts the fetched data Tensors in pinned memory, and enables faster data transfer to CUDA-enabled GPUs”\n\n**num_workers:** PyTorch allows loading data on multiple processes simultaneously. A good rule: ***num_worker = 4 * num_GPU***","metadata":{}},{"cell_type":"code","source":"bs = 50\n\noriginal = ImageFolder(path, transform=transformer['original'])\n\n#all_set = train_val + test\ntrain_val, test = train_test_split(original, test_size=0.2, shuffle=True, random_state=43)\n\n#train_val = train + val + dataset1 + dataset2 + dataset3\ntrain_val = ConcatDataset([train_val, \n                           ImageFolder(path, transform=transformer['all-mias']),\n                           ImageFolder(path, transform=transformer['all-mias']),\n                           ImageFolder(path, transform=transformer['all-mias'])]) \n\ntrain, val = train_test_split(train_val, test_size=0.1, shuffle=True, random_state=43)\n\nloaders = {\n    'train': DataLoader(train, batch_size=bs, num_workers=4, pin_memory=True),\n    'val': DataLoader(val, batch_size=bs, num_workers=4, pin_memory=True),\n    'test': DataLoader(test, batch_size=bs, num_workers=4, pin_memory=True)\n}\n\ndataset_sizes = {\n    'train': len(train),\n    'val': len(val), \n    'test': len(test),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:16:05.218244Z","iopub.execute_input":"2022-05-07T12:16:05.218607Z","iopub.status.idle":"2022-05-07T12:16:05.247036Z","shell.execute_reply.started":"2022-05-07T12:16:05.218574Z","shell.execute_reply":"2022-05-07T12:16:05.245246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have already normalized the data, but at this stage I would like to dwell in more detail, because this is very important.**\n\nIn datasets, we have three-channel images, that is, we need to normalize for each channel separately (!!!). Because of the unnormalized data, problems may appear, for example, regularization during training can work to the detriment, but we do not want this at all. The task of normalization is to make the mean as close to zero as possible, and the standard deviation around 1. \n\nHow each channel looks separately can be seen below:","metadata":{}},{"cell_type":"code","source":"exampleset = ImageFolder(path, transform=transform.Compose([transform.ToTensor(),\n                                                            transform.CenterCrop(255),]))\n\nx, y = next(iter(DataLoader(exampleset)))\n\nchannels = ['Red', 'Green', 'Blue']\ncmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 10))\n\nfor i, axs in enumerate(fig.axes[:3]):\n    axs.imshow(x[0][i,:,:], cmap=cmaps[i])\n    axs.set_title(f'{channels[i]} Channel')\n    axs.set_xticks([])\n    axs.set_yticks([])\n    \nax[3].imshow(x[0].permute(1,2,0))\nax[3].set_title('Three Channels')\nax[3].set_xticks([])\nax[3].set_yticks([]);","metadata":{"execution":{"iopub.status.busy":"2021-06-12T07:57:34.397226Z","iopub.execute_input":"2021-06-12T07:57:34.397536Z","iopub.status.idle":"2021-06-12T07:57:34.835673Z","shell.execute_reply.started":"2021-06-12T07:57:34.397508Z","shell.execute_reply":"2021-06-12T07:57:34.834876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let's check how well we managed to normalize the data for each channel for the test, training and validation datasets:**","metadata":{}},{"cell_type":"code","source":"channels = 3\n\nfor channel in range(channels):\n    for x in ['train', 'val', 'test']:\n        #number of pixels in the dataset = number of all pixels in one object * number of all objects in the dataset\n        num_pxl = dataset_sizes[x]*220*220\n    \n        #we go through the butches and sum up the pixels of the objects, \n        #which then divide the sum by the number of all pixels to calculate the average\n        total_sum = 0\n        for batch in loaders[x]:\n            layer = list(map(itemgetter(channel), batch[0]))\n            layer = torch.stack(layer, dim=0)\n            total_sum += layer.sum()\n        mean = total_sum / num_pxl\n\n        #we calculate the standard deviation using the formula that I indicated above\n        sum_sqrt = 0\n        for batch in loaders[x]: \n            layer = list(map(itemgetter(channel), batch[0]))\n            sum_sqrt += ((torch.stack(layer, dim=0) - mean).pow(2)).sum()\n        std = torch.sqrt(sum_sqrt / num_pxl)\n        \n        print(f'|channel:{channel+1}| {x} - mean: {mean}, std: {std}')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T07:59:17.600397Z","iopub.execute_input":"2021-06-12T07:59:17.600731Z","iopub.status.idle":"2021-06-12T08:00:28.383426Z","shell.execute_reply.started":"2021-06-12T07:59:17.600689Z","shell.execute_reply":"2021-06-12T08:00:28.382384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's take a batch from the training dataset and see its mean and standard deviation:**","metadata":{}},{"cell_type":"code","source":"x, y = next(iter(loaders['train']))\nx.mean(),  x.std()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:00:32.965083Z","iopub.execute_input":"2021-06-12T08:00:32.965413Z","iopub.status.idle":"2021-06-12T08:00:33.78305Z","shell.execute_reply.started":"2021-06-12T08:00:32.965381Z","shell.execute_reply":"2021-06-12T08:00:33.782117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's take a look at the pixel distribution after normalization. Compared to the distribution at the beginning, the difference is large**","metadata":{}},{"cell_type":"code","source":"x, y = next(iter(loaders['train']))\nimg_norm = x[0].permute(1,2,0).numpy()\nplotHist(img_norm)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:00:36.476266Z","iopub.execute_input":"2021-06-12T08:00:36.476589Z","iopub.status.idle":"2021-06-12T08:00:37.431184Z","shell.execute_reply.started":"2021-06-12T08:00:36.476558Z","shell.execute_reply":"2021-06-12T08:00:37.43033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is how we can look at our classes. There are only five of them, which is not much**","metadata":{}},{"cell_type":"code","source":"print('Classes:', original.classes)\nprint('Number of classes:', len(original.classes))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:00:40.325845Z","iopub.execute_input":"2021-06-12T08:00:40.326187Z","iopub.status.idle":"2021-06-12T08:00:40.333523Z","shell.execute_reply.started":"2021-06-12T08:00:40.326154Z","shell.execute_reply":"2021-06-12T08:00:40.332553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check how imbalanced our data is. Since we are working with images, first we need to make a pandas table, and then render the table.**","metadata":{}},{"cell_type":"code","source":"dic = {}\n\nfor classes in original.classes:\n  dic[classes] = [len([os.path.join(path+'/'+classes, filename) for filename in os.listdir(path+'/'+classes)])]\n\nsamplesize = pd.DataFrame.from_dict(dic)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:06.124689Z","iopub.execute_input":"2021-06-12T08:01:06.125069Z","iopub.status.idle":"2021-06-12T08:01:06.148432Z","shell.execute_reply.started":"2021-06-12T08:01:06.125037Z","shell.execute_reply":"2021-06-12T08:01:06.147718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samplesize","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:08.053396Z","iopub.execute_input":"2021-06-12T08:01:08.053775Z","iopub.status.idle":"2021-06-12T08:01:08.075876Z","shell.execute_reply.started":"2021-06-12T08:01:08.053725Z","shell.execute_reply":"2021-06-12T08:01:08.074978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The imbalance is small and we do not need to handle it in any way, since the ratio of the largest class to the smallest is 1.47, which is not much**","metadata":{}},{"cell_type":"code","source":"figure_size = plt.rcParams['figure.figsize']\nfigure_size[0] = 40\nfigure_size[1] = 20\nplt.rcParams['figure.figsize'] = figure_size\n\nsns.barplot(data=samplesize)\n\nindex = np.arange(len(original.classes))\n\nplt.xlabel('Flowers', fontsize=25)\nplt.ylabel('Count of flowers', fontsize=25)\nplt.xticks(index, original.classes, fontsize=25)\nplt.title('Class Distrubution', fontsize=35)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:10.448351Z","iopub.execute_input":"2021-06-12T08:01:10.448668Z","iopub.status.idle":"2021-06-12T08:01:10.82832Z","shell.execute_reply.started":"2021-06-12T08:01:10.448639Z","shell.execute_reply":"2021-06-12T08:01:10.827515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see how the images from the original dataset look like without changes:**","metadata":{}},{"cell_type":"code","source":"# Function for plotting samples\ndef plot_samples(samples):  \n    fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(15,12))\n    i = 0\n    for row in range(5):\n         for col in range(5):\n                img = mpimg.imread(samples[i][0][0])\n                ax[row][col].imshow(img)\n                ax[row][col].axis('off')\n                ax[row][col].set_title(samples[i][1], fontsize=15)\n                i+=1\n  \n\nrand_samples = [] \nfor _ in range(25): \n    classes = random.choice(original.classes)\n    rand_samples.append([random.sample([os.path.join(path+'/'+classes, filename) for filename in os.listdir(path+'/'+classes)], 1), classes]) \nrand_samples[0]\nplot_samples(rand_samples)\nplt.suptitle('Samples', fontsize=30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:14.69229Z","iopub.execute_input":"2021-06-12T08:01:14.692613Z","iopub.status.idle":"2021-06-12T08:01:16.296597Z","shell.execute_reply.started":"2021-06-12T08:01:14.692581Z","shell.execute_reply":"2021-06-12T08:01:16.295647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And this is how images with augmentation look like**","metadata":{}},{"cell_type":"code","source":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(25, 25))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:60], nrow=10).permute(1, 2, 0))\n        ax.set_title('Images with augmentation', fontsize=40)\n        break\n        \nshow_batch(loaders['train'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:20.954445Z","iopub.execute_input":"2021-06-12T08:01:20.954795Z","iopub.status.idle":"2021-06-12T08:01:22.837717Z","shell.execute_reply.started":"2021-06-12T08:01:20.954744Z","shell.execute_reply":"2021-06-12T08:01:22.836689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training and Test\n\n**Idea:** I will use an ensemble of pre-trained models. I first train only the classifier on 10 epochs, then unfreeze the network and train all together for another 10 epochs","metadata":{}},{"cell_type":"markdown","source":"In order not to count the accuracy many times, we write the function","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1) \n    return torch.tensor(torch.sum(preds == labels).item() / len(preds)), preds","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:34.053192Z","iopub.execute_input":"2021-06-12T08:01:34.053533Z","iopub.status.idle":"2021-06-12T08:01:34.05957Z","shell.execute_reply.started":"2021-06-12T08:01:34.0535Z","shell.execute_reply":"2021-06-12T08:01:34.058653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is where we will record the history of learning, so that we can make visualization later. We need visualization to evaluate learning, for example, overfitting or underfitting. Of course, we can analyze with numbers, but it is much easier to perceive information visually**","metadata":{}},{"cell_type":"code","source":"#save the losses for further visualization\nlosses = {'train':[], 'val':[]}\naccuracies = {'train':[], 'val':[]}\nlr = []","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:34.13011Z","iopub.execute_input":"2021-06-12T08:01:34.130367Z","iopub.status.idle":"2021-06-12T08:01:34.134122Z","shell.execute_reply.started":"2021-06-12T08:01:34.130344Z","shell.execute_reply":"2021-06-12T08:01:34.133255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train function structure:\n\n1. **Classifier Training**\n2. **Network-wide Training**","metadata":{}},{"cell_type":"code","source":"def train(seed, epochs, model):\n    \n  print('Creating a model {}...'.format(seed))\n\n  model.to(device)  \n  criterion = nn.CrossEntropyLoss()\n  if seed==2 or seed==3:\n    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay = 1e-5)\n  else:\n    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n  #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.1, epochs=epochs, steps_per_epoch=len(loaders['train']), cycle_momentum=True)\n  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)\n  since = time.time()\n  best_model = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n#             lr.append(scheduler.get_lr())\n#             scheduler.step()\n\n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n      if phase == 'train':\n          acc = 100. * running_corrects.double() / dataset_sizes[phase]\n          scheduler.step(acc)\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double()/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n      lr.append(scheduler._last_lr)\n        \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)//60, (time.time()- since)%60))\n        print('=='*31)\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    #scheduler.step() \n  time_elapsed = time.time() - since\n  print('CLASSIFIER TRAINING TIME {}m {}s'.format(time_elapsed//60, time_elapsed%60))\n  print('=='*31)\n\n\n  model.load_state_dict(best_model)\n\n  for param in model.parameters():\n        param.requires_grad=True\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)  \n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)\n  #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.001, epochs=epochs, steps_per_epoch=len(loaders['train']), cycle_momentum=True)\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n#             lr.append(scheduler.get_lr())\n#             scheduler.step()\n\n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n      if phase == 'train':\n        acc = 100. * running_corrects.double() / dataset_sizes[phase]\n        scheduler.step(acc)\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double()/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n      lr.append(scheduler._last_lr)\n    \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)//60, (time.time()- since)%60))\n        print('=='*31)    \n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    #scheduler.step() \n  time_elapsed = time.time() - since\n  print('ALL NET TRAINING TIME {}m {}s'.format(time_elapsed//60, time_elapsed%60))\n  print('=='*31)\n\n  model.load_state_dict(best_model)\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:34.209417Z","iopub.execute_input":"2021-06-12T08:01:34.209654Z","iopub.status.idle":"2021-06-12T08:01:34.23365Z","shell.execute_reply.started":"2021-06-12T08:01:34.209631Z","shell.execute_reply":"2021-06-12T08:01:34.232714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODELS","metadata":{}},{"cell_type":"markdown","source":"**1. DenseNet(1)**","metadata":{}},{"cell_type":"code","source":"densenet121_0 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_0.parameters():\n  param.requires_grad=False\n\ndensenet121_0.classifier = nn.Linear(in_features=densenet121_0.classifier.in_features, out_features=len(original.classes), bias=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:34.268853Z","iopub.execute_input":"2021-06-12T08:01:34.269088Z","iopub.status.idle":"2021-06-12T08:01:39.495432Z","shell.execute_reply.started":"2021-06-12T08:01:34.269066Z","shell.execute_reply":"2021-06-12T08:01:39.494459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. DenseNet(2)**","metadata":{}},{"cell_type":"code","source":"densenet121_1 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_1.parameters():\n  param.requires_grad=False\n\ndensenet121_1.classifier = nn.Linear(in_features=densenet121_1.classifier.in_features, out_features=len(original.classes), bias=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:39.497993Z","iopub.execute_input":"2021-06-12T08:01:39.498646Z","iopub.status.idle":"2021-06-12T08:01:39.946685Z","shell.execute_reply.started":"2021-06-12T08:01:39.498603Z","shell.execute_reply":"2021-06-12T08:01:39.945805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. GoogleNet**","metadata":{}},{"cell_type":"code","source":"googlenet = torchvision.models.googlenet(pretrained=True)\nfor param in googlenet.parameters():\n  param.grad_requires = False\n\ngooglenet.fc = nn.Linear(in_features=googlenet.fc.in_features, out_features=len(original.classes), bias=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:39.94849Z","iopub.execute_input":"2021-06-12T08:01:39.948854Z","iopub.status.idle":"2021-06-12T08:01:46.735081Z","shell.execute_reply.started":"2021-06-12T08:01:39.948817Z","shell.execute_reply":"2021-06-12T08:01:46.734238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. ResNet**","metadata":{}},{"cell_type":"code","source":"resnet101 = torchvision.models.resnet101(pretrained=True)\nfor param in resnet101.parameters():\n  param.grad_requires = False\n\nresnet101.fc = nn.Linear(in_features=resnet101.fc.in_features, out_features=len(original.classes), bias=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:01:46.738231Z","iopub.execute_input":"2021-06-12T08:01:46.738484Z","iopub.status.idle":"2021-06-12T08:02:05.208149Z","shell.execute_reply.started":"2021-06-12T08:01:46.738459Z","shell.execute_reply":"2021-06-12T08:02:05.207256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. VGG19**","metadata":{}},{"cell_type":"code","source":"vgg19_bn = torchvision.models.vgg19_bn(pretrained=True)\nfor param in vgg19_bn.parameters():\n  param.grad_requires = False\n\nvgg19_bn.classifier[6] = nn.Linear(4096, len(original.classes), bias=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:02:05.209409Z","iopub.execute_input":"2021-06-12T08:02:05.2098Z","iopub.status.idle":"2021-06-12T08:02:58.782349Z","shell.execute_reply.started":"2021-06-12T08:02:05.20975Z","shell.execute_reply":"2021-06-12T08:02:58.775848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Launching training**","metadata":{}},{"cell_type":"code","source":"num_models = 5\nepochs = 10\n\nmodels = [densenet121_0, densenet121_1, googlenet, resnet101, vgg19_bn]\n\nfor seed in range(num_models):\n   train(seed=seed, epochs=epochs, model=models[seed])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:02:58.788521Z","iopub.execute_input":"2021-06-12T08:02:58.788925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Loss and Accuracy Plots","metadata":{}},{"cell_type":"markdown","source":"**As you can see from the graphs, the unfreeze idea worked. We can see that after the red lines, the performance improves!**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 2, figsize=(15, 15))\nmodelname = ['DenseNet_0', 'DenseNet_1', 'GooglNet', 'ResNet101', 'VGG16 with BN']\n\ni=0\n\nfor row in range(5):\n\n  epoch_list = list(range(1,epochs*2+1))\n\n  ax[row][0].plot(epoch_list, accuracies['train'][i:20+i], '-o', label='Train Accuracy')\n  ax[row][0].plot(epoch_list, accuracies['val'][i:20+i], '-o', label='Validation Accuracy')\n  ax[row][0].plot([epochs for x in range(20)],  np.linspace(min(accuracies['train'][i:20+i]).cpu(), max(accuracies['train'][i:20+i]).cpu(), 20), color='r', label='Unfreeze net')\n  ax[row][0].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][0].set_ylabel('Accuracy Value')\n  ax[row][0].set_xlabel('Epoch')\n  ax[row][0].set_title('Accuracy {}'.format(modelname[row]))\n  ax[row][0].legend(loc=\"best\")\n\n  ax[row][1].plot(epoch_list, losses['train'][i:20+i], '-o', label='Train Loss')\n  ax[row][1].plot(epoch_list, losses['val'][i:20+i], '-o',label='Validation Loss')\n  ax[row][1].plot([epochs for x in range(20)], np.linspace(min(losses['train'][i:20+i]), max(losses['train'][i:20+i]), 20), color='r', label='Unfreeze net')\n  ax[row][1].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][1].set_ylabel('Loss Value')\n  ax[row][1].set_xlabel('Epoch')\n  ax[row][1].set_title('Loss {}'.format(modelname[row]))\n  ax[row][1].legend(loc=\"best\")\n  fig.tight_layout()\n  fig.subplots_adjust(top=1.5, wspace=0.3)\n\n  i+=20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Test set predictions","metadata":{}},{"cell_type":"markdown","source":"**First, let's write an ensemble class. It's very easy!**","metadata":{}},{"cell_type":"code","source":"class Ensemble(nn.Module):\n    def __init__(self, device):\n        super(Ensemble,self).__init__()\n        # you should use nn.ModuleList. Optimizer doesn't detect python list as parameters\n        self.models = nn.ModuleList(models)\n        \n    def forward(self, x):\n        # it is super simple. just forward num_ models and concat it.\n        output = torch.zeros([x.size(0), len(original.classes)]).to(device)\n        for model in self.models:\n            output += model(x)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =  Ensemble(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**These functions will help us when calculating the accuracy**","metadata":{}},{"cell_type":"code","source":"def validation_step(batch):\n        images,labels = batch\n        images,labels = images.to(device),labels.to(device)\n        out = model(images)                                      \n        loss = F.cross_entropy(out, labels)                    \n        acc,preds = accuracy(out, labels)                       \n        \n        return {'val_loss': loss.detach(), 'val_acc':acc.detach(), \n                'preds':preds.detach(), 'labels':labels.detach()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_prediction(outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()           \n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()             \n        # combine predictions\n        batch_preds = [pred for x in outputs for pred in x['preds'].tolist()] \n        # combine labels\n        batch_labels = [lab for x in outputs for lab in x['labels'].tolist()]  \n        \n        return {'test_loss': epoch_loss.item(), 'test_acc': epoch_acc.item(),\n                'test_preds': batch_preds, 'test_labels': batch_labels}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef test_predict(model, test_loader):\n    model.eval()\n    # perform testing for each batch\n    outputs = [validation_step(batch) for batch in test_loader] \n    results = test_prediction(outputs)                          \n    print('test_loss: {:.4f}, test_acc: {:.4f}'\n          .format(results['test_loss'], results['test_acc']))\n    \n    return results['test_preds'], results['test_labels']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It seems to me that we have achieved good enough accuracy**","metadata":{}},{"cell_type":"code","source":"model.to(device)\npreds,labels = test_predict(model, loaders['test'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Predictions in individual images","metadata":{}},{"cell_type":"markdown","source":"**The function below will normalize the image back to its original. It simply multiplies the tensor by the standard deviation and adds the mean**","metadata":{}},{"cell_type":"code","source":"def norm_out(img):\n    \n    img = img.permute(1,2,0)\n    mean = torch.FloatTensor([0.4124234616756439, 0.3674212694168091, 0.2578217089176178])\n    std = torch.FloatTensor([0.3268945515155792, 0.29282665252685547, 0.29053378105163574])\n    \n    img = img*std + mean\n        \n    return np.clip(img,0,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In random pictures, the network shows 100% accuracy, this is the most confident answer, so you can make the assumption that the networks have learned well**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,12), ncols=2, nrows=4)\n\nfor row in range(4):\n    i = np.random.randint(0, high=len(test))\n    img,label = test[i]\n    \n    m = nn.Softmax(dim=1)\n    percent = m(model(img.to(device).unsqueeze(0)))\n    predmax3percent = torch.sort(percent[0])[0]\n    predmax3inds = torch.sort(percent[0])[1]\n    classes = np.array([original.classes[predmax3inds[-5]], original.classes[predmax3inds[-4]], original.classes[predmax3inds[-3]], original.classes[predmax3inds[-2]],original.classes[predmax3inds[-1]]])\n    class_name = original.classes\n\n    ax[row][0].imshow(norm_out(img))\n    ax[row][0].set_title('Real : {}'.format(class_name[label]))\n    ax[row][0].axis('off')\n    ax[row][1].barh(classes, predmax3percent.detach().cpu().numpy())\n    ax[row][1].set_aspect(0.1)\n    ax[row][1].set_yticks(classes)\n    ax[row][1].set_title('Predicted Class: {} ({}%)'.format(original.classes[predmax3inds[-1]], round((predmax3percent[-1]*100).item(), 2)))\n    ax[row][1].set_xlim(0, 1.)\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Metrics","metadata":{}},{"cell_type":"code","source":"report = classification_report(labels, preds,\n                               output_dict=True,\n                               target_names=original.classes)\nreport_df = pd.DataFrame(report).transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None)\nreport_df.head(134)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot confusion matrix\ncm  = confusion_matrix(labels, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\nplt.xticks(range(len(original.classes)), original.classes, fontsize=16)\nplt.yticks(range(len(original.classes)), original.classes, fontsize=16)\nplt.xlabel('Predicted Label',fontsize=18)\nplt.ylabel('True Label',fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***I hope you enjoyed it and found something new for yourself!*\n*I am always happy to receive any feedback. What do you think can be changed and what can be removed?***","metadata":{}},{"cell_type":"markdown","source":"**This might be helpful:**\n\nPredicting pneumonia by X-ray: https://www.kaggle.com/georgiisirotenko/pytorch-x-ray-transfer-learning-densenet\n\nFruit prediction for 131 classes(!!!): https://www.kaggle.com/georgiisirotenko/pytorch-fruits-transferlearing-ensemble-test99-18\n\nFashionMNIST: https://www.kaggle.com/georgiisirotenko/pytorch-fashionmnist-acc-0-94\n\nA similar solution, but with a submission(MNIST top 5%): https://www.kaggle.com/georgiisirotenko/pytorch-mnist-transferlearning-ensemble-99-714","metadata":{}}]}